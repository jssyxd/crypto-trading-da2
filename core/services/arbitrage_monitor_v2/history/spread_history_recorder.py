"""
å†å²æ•°æ®è®°å½•å™¨

èŒè´£ï¼š
- æ—¶é—´çª—å£é‡‡æ ·
- å¼‚æ­¥æ‰¹é‡å†™å…¥CSVæ–‡ä»¶
- å®Œå…¨å¼‚æ­¥åŒ–ï¼Œä¸é˜»å¡æ ¸å¿ƒæµç¨‹

æ€§èƒ½ä¿è¯ï¼š
- å†…å­˜å†™å…¥ï¼š< 0.001ms
- é‡‡æ ·æ“ä½œï¼š< 0.1msï¼ˆæ¯1åˆ†é’Ÿä¸€æ¬¡ï¼‰
- é˜Ÿåˆ—æ“ä½œï¼š< 0.001ms
- æ€»ä½“å½±å“ï¼š< 0.01ms
"""

import asyncio
import time
import logging
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

# ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿé…ç½®ï¼ˆå‚è€ƒç½‘æ ¼ç³»ç»Ÿï¼‰
from core.adapters.exchanges.utils.setup_logging import LoggingConfig

# ğŸ”¥ é…ç½®æ—¥å¿—è®°å½•å™¨ï¼ˆå†™å…¥æ–‡ä»¶ï¼‰
logger = LoggingConfig.setup_logger(
    name=__name__,
    log_file='spread_history.log',
    console_formatter=None,  # ä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé¿å…å¹²æ‰°UI
    file_formatter='detailed',
    level=logging.INFO
)
logger.propagate = False

try:
    import aiofiles
except ImportError:
    aiofiles = None
    logger.warning("âš ï¸  aiofilesæœªå®‰è£…ï¼Œå†å²è®°å½•åŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚è¯·è¿è¡Œ: pip install aiofiles")

try:
    import aiosqlite
except ImportError:
    aiosqlite = None
    logger.warning("âš ï¸  aiosqliteæœªå®‰è£…ï¼ŒSQLiteåŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚è¯·è¿è¡Œ: pip install aiosqlite")


class SpreadHistoryRecorder:
    """å†å²è®°å½•å™¨ï¼ˆå®Œå…¨å¼‚æ­¥ï¼Œä¸é˜»å¡æ ¸å¿ƒæµç¨‹ï¼‰
    
    é‡‡ç”¨æ—¶é—´çª—å£é‡‡æ ·ç­–ç•¥ï¼šæ¯1åˆ†é’Ÿé‡‡æ ·ä¸€æ¬¡æ•°æ®ç‚¹
    """
    
    def __init__(
        self, 
        data_dir: str = "data/spread_history",
        sample_interval_seconds: int = 60,
        sample_strategy: str = "max",
        batch_size: int = 10,
        batch_timeout: float = 60.0,
        queue_maxsize: int = 500,
        compress_after_days: int = 10,
        archive_after_days: int = 30,
        cleanup_interval_hours: int = 24,
        db_retention_hours: int = 48
    ):
        """
        åˆå§‹åŒ–å†å²è®°å½•å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
            sample_interval_seconds: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1åˆ†é’Ÿ
            sample_strategy: é‡‡æ ·ç­–ç•¥ï¼šmax(æœ€å¤§å€¼), mean(å¹³å‡å€¼), latest(æœ€æ–°å€¼)
            batch_size: æ‰¹é‡å†™å…¥å¤§å°
            batch_timeout: æ‰¹é‡å†™å…¥è¶…æ—¶ï¼ˆç§’ï¼‰
            queue_maxsize: å†™å…¥é˜Ÿåˆ—æœ€å¤§å¤§å°
            compress_after_days: å‹ç¼©å¤©æ•°ï¼ˆ10å¤©åå‹ç¼©ï¼‰
            archive_after_days: å½’æ¡£å¤©æ•°ï¼ˆ30å¤©åå½’æ¡£ï¼‰
            cleanup_interval_hours: æ¸…ç†ä»»åŠ¡æ‰§è¡Œé—´éš”ï¼ˆå°æ—¶ï¼‰
            db_retention_hours: æ•°æ®åº“ä¿ç•™æ—¶é•¿ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤48å°æ—¶
        """
        if aiofiles is None:
            raise ImportError("aiofilesæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å†å²è®°å½•åŠŸèƒ½")
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        (self.data_dir / "raw").mkdir(parents=True, exist_ok=True)
        (self.data_dir / "archive").mkdir(parents=True, exist_ok=True)
        
        # SQLiteæ•°æ®åº“è·¯å¾„
        self.db_path = self.data_dir / "spread_history.db"
        self.sqlite_enabled = aiosqlite is not None
        
        # å‹ç¼©å’Œå½’æ¡£é…ç½®
        self.compress_after_days = compress_after_days
        self.archive_after_days = archive_after_days
        self.cleanup_interval_hours = cleanup_interval_hours
        
        # ğŸ”¥ æ•°æ®åº“æ¸…ç†é…ç½®ï¼ˆä¿æŒæ•°æ®åº“æ€§èƒ½ï¼‰
        self.db_retention_hours = db_retention_hours
        
        # æ—¶é—´çª—å£é‡‡æ ·é…ç½®
        self.sample_interval_seconds = sample_interval_seconds
        self.sample_strategy = sample_strategy
        
        # æ—¶é—´çª—å£æ•°æ®ç¼“å­˜ï¼ˆç”¨äºé‡‡æ ·ï¼Œé™åˆ¶å¤§å°ï¼‰
        self.window_data: Dict[str, List[dict]] = defaultdict(list)
        self.window_data_maxsize = 100  # æ¯ä¸ªä»£å¸æœ€å¤šä¿ç•™100æ¡
        self.last_sample_time = time.time()
        
        # ğŸ”¥ æ¯ä¸ªä»·å·®æ–¹å‘çš„æœ€åè®°å½•æ—¶é—´ï¼ˆç”¨äºå»é‡ï¼Œ1åˆ†é’Ÿå†…åªè®°å½•ä¸€æ¬¡ï¼‰
        # ğŸ”¥ ä¿®æ”¹ï¼šåŒä¸€ä¸ªä»£å¸å¯èƒ½æœ‰2ä¸ªæ–¹å‘çš„ä»·å·®ï¼ˆex1ä¹°->ex2å– å’Œ ex2ä¹°->ex1å–ï¼‰ï¼Œéœ€è¦åˆ†åˆ«å»é‡
        # é”®æ ¼å¼ï¼šf"{symbol}_{exchange_buy}_{exchange_sell}"
        self._last_record_time: Dict[str, float] = {}
        self._record_interval_seconds = 60  # æ¯ä¸ªä»·å·®æ–¹å‘1åˆ†é’Ÿå†…åªè®°å½•ä¸€æ¬¡
        
        # å†™å…¥é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼‰
        self.write_queue = asyncio.Queue(maxsize=queue_maxsize)
        
        # å½“å‰æ—¥æœŸï¼ˆç”¨äºæ–‡ä»¶è½®è½¬ï¼‰
        self.current_date = datetime.now().date()
        self.current_file: Optional[Path] = None
        
        # æ‰¹é‡å†™å…¥é…ç½®
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        
        # å†™å…¥ä»»åŠ¡
        self.write_task: Optional[asyncio.Task] = None
        self.sample_task: Optional[asyncio.Task] = None
        self.cleanup_task: Optional[asyncio.Task] = None
        
        # è¿è¡ŒçŠ¶æ€
        self.running = False
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'records_received': 0,
            'samples_taken': 0,
            'batches_written': 0,
            'sqlite_batches_written': 0,
            'queue_drops': 0,
            'files_compressed': 0,
            'files_archived': 0,
        }
    
    async def start(self):
        """å¯åŠ¨å¼‚æ­¥å†™å…¥ä»»åŠ¡å’Œé‡‡æ ·ä»»åŠ¡"""
        if self.running:
            return
        
        # åˆå§‹åŒ–SQLiteæ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.sqlite_enabled:
            await self._init_sqlite_database()
        
        self.running = True
        self.write_task = asyncio.create_task(self._async_write_loop())
        self.sample_task = asyncio.create_task(self._time_window_sampler())
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        sqlite_status = "ï¼ˆSQLiteå·²å¯ç”¨ï¼‰" if self.sqlite_enabled else "ï¼ˆSQLiteæœªå¯ç”¨ï¼‰"
        logger.info(f"âœ… [å†å²è®°å½•] å†å²è®°å½•å™¨å·²å¯åŠ¨ {sqlite_status}")
        logger.info(f"ğŸ“ [å†å²è®°å½•] æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"â±ï¸  [å†å²è®°å½•] é‡‡æ ·é—´éš”: {self.sample_interval_seconds}ç§’ï¼Œé‡‡æ ·ç­–ç•¥: {self.sample_strategy}")
        logger.info(f"ğŸ’¾ [å†å²è®°å½•] æ‰¹é‡å†™å…¥é…ç½®: batch_size={self.batch_size}, batch_timeout={self.batch_timeout}ç§’")
        logger.info(f"ğŸ—‘ï¸  [å†å²è®°å½•] æ•°æ®åº“ä¿ç•™æ—¶é•¿: {self.db_retention_hours}å°æ—¶ï¼ˆè¶…è¿‡æ­¤æ—¶é•¿çš„æ•°æ®å°†è¢«è‡ªåŠ¨æ¸…ç†ï¼‰")
        logger.info(f"ğŸ“¦ [å†å²è®°å½•] æ¸…ç†ä»»åŠ¡é—´éš”: {self.cleanup_interval_hours}å°æ—¶")
        # ğŸ”¥ è¯Šæ–­ï¼šç¡®è®¤æ—¥å¿—é…ç½®
        logger.info(f"ğŸ” [å†å²è®°å½•] æ—¥å¿—é…ç½®æ£€æŸ¥: handlers={[type(h).__name__ for h in logger.handlers]}, level={logger.level}")
    
    async def stop(self):
        """åœæ­¢å†å²è®°å½•å™¨"""
        self.running = False
        
        if self.sample_task:
            self.sample_task.cancel()
            try:
                await self.sample_task
            except asyncio.CancelledError:
                pass
        
        if self.write_task:
            self.write_task.cancel()
            try:
                await self.write_task
            except asyncio.CancelledError:
                pass
        
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
        
        # å†™å…¥å‰©ä½™æ•°æ®
        await self._flush_remaining_data()
        
        logger.info("ğŸ›‘ [å†å²è®°å½•] å†å²è®°å½•å™¨å·²åœæ­¢")
    
    async def record_spread(self, data: dict):
        """è®°å½•ä»·å·®ï¼ˆéé˜»å¡ï¼Œåªå†™å…¥æ—¶é—´çª—å£ç¼“å­˜ï¼‰
        
        æ€§èƒ½ä¿è¯ï¼š< 0.001msï¼Œä¸é˜»å¡
        
        ğŸ”¥ å»é‡æœºåˆ¶ï¼šæ¯ä¸ªä»£å¸åœ¨1åˆ†é’Ÿå†…åªè®°å½•ä¸€æ¬¡
        
        Args:
            data: å¥—åˆ©æœºä¼šæ•°æ®ï¼ŒåŒ…å«ï¼š
                - symbol: ä»£å¸ç¬¦å·
                - exchange_buy: ä¹°å…¥äº¤æ˜“æ‰€
                - exchange_sell: å–å‡ºäº¤æ˜“æ‰€
                - price_buy: ä¹°å…¥ä»·æ ¼
                - price_sell: å–å‡ºä»·æ ¼
                - spread_pct: ä»·å·®ç™¾åˆ†æ¯”ï¼ˆä¸»è¦æ•°æ®ï¼‰
                - funding_rate_buy: ä¹°å…¥äº¤æ˜“æ‰€èµ„é‡‘è´¹ç‡ï¼ˆä¸»è¦æ•°æ®ï¼‰
                - funding_rate_sell: å–å‡ºäº¤æ˜“æ‰€èµ„é‡‘è´¹ç‡ï¼ˆä¸»è¦æ•°æ®ï¼‰
                - funding_rate_diff: èµ„é‡‘è´¹ç‡å·®ï¼ˆä¸»è¦æ•°æ®ï¼Œ8å°æ—¶è´¹ç‡å·®ï¼‰
                - funding_rate_diff_annual: å¹´åŒ–èµ„é‡‘è´¹ç‡å·®ï¼ˆå¯é€‰ï¼‰
                - size_buy: ä¹°å…¥æ•°é‡ï¼ˆå¯é€‰ï¼‰
                - size_sell: å–å‡ºæ•°é‡ï¼ˆå¯é€‰ï¼‰
        """
        if not self.running:
            logger.warning(f"âš ï¸  [å†å²è®°å½•] å†å²è®°å½•å™¨æœªè¿è¡Œï¼Œè·³è¿‡è®°å½•: {data.get('symbol', 'unknown')}")
            return
        
        symbol = data.get('symbol')
        exchange_buy = data.get('exchange_buy')
        exchange_sell = data.get('exchange_sell')
        if not symbol or not exchange_buy or not exchange_sell:
            return
        
        # ğŸ”¥ å»é‡æ£€æŸ¥ï¼šæ¯ä¸ªä»·å·®æ–¹å‘åœ¨1åˆ†é’Ÿå†…åªè®°å½•ä¸€æ¬¡
        # ğŸ”¥ ä¿®æ”¹ï¼šåŒä¸€ä¸ªä»£å¸å¯èƒ½æœ‰2ä¸ªæ–¹å‘çš„ä»·å·®ï¼Œéœ€è¦åˆ†åˆ«å»é‡
        # é”®æ ¼å¼ï¼šf"{symbol}_{exchange_buy}_{exchange_sell}"
        spread_key = f"{symbol}_{exchange_buy}_{exchange_sell}"
        current_time = time.time()
        last_record_time = self._last_record_time.get(spread_key, 0)
        if current_time - last_record_time < self._record_interval_seconds:
            # åœ¨1åˆ†é’Ÿå†…å·²è®°å½•è¿‡ï¼Œè·³è¿‡ï¼ˆé™é»˜è·³è¿‡ï¼Œä¸è®°å½•æ—¥å¿—ï¼‰
            return
        
        # æ›´æ–°æœ€åè®°å½•æ—¶é—´
        self._last_record_time[spread_key] = current_time
        
        # æ·»åŠ åˆ°æ—¶é—´çª—å£ç¼“å­˜ï¼ˆä¸é˜»å¡ï¼‰
        # ğŸ”¥ ä½¿ç”¨symbolä½œä¸ºkeyï¼Œå› ä¸ºåŒä¸€ä¸ªsymbolçš„2ä¸ªæ–¹å‘ä»·å·®éœ€è¦ä¸€èµ·é‡‡æ ·
        self.window_data[symbol].append({
            **data,
            'timestamp': datetime.now().isoformat()
        })
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼ˆé¿å…å†…å­˜ç§¯ç´¯ï¼‰
        if len(self.window_data[symbol]) > self.window_data_maxsize:
            self.window_data[symbol] = self.window_data[symbol][-self.window_data_maxsize:]
        
        self.stats['records_received'] += 1
        
        # ğŸ”¥ è°ƒè¯•æ—¥å¿—ï¼šæ¯100æ¡è®°å½•è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
        if self.stats['records_received'] % 100 == 0:
            total_cached = sum(len(v) for v in self.window_data.values())
            logger.info(f"ğŸ“Š [å†å²è®°å½•] å·²æ¥æ”¶ {self.stats['records_received']} æ¡è®°å½•ï¼Œå†…å­˜ç¼“å­˜ {total_cached} æ¡ï¼Œç­‰å¾…é‡‡æ ·...")
    
    async def _time_window_sampler(self):
        """æ—¶é—´çª—å£é‡‡æ ·å™¨ï¼ˆæ¯1åˆ†é’Ÿé‡‡æ ·ä¸€æ¬¡ï¼Œéé˜»å¡ï¼‰"""
        logger.info(f"ğŸ• [å†å²è®°å½•] é‡‡æ ·å™¨å·²å¯åŠ¨ï¼Œé‡‡æ ·é—´éš”: {self.sample_interval_seconds}ç§’")
        while self.running:
            try:
                await asyncio.sleep(self.sample_interval_seconds)
                
                current_time = time.time()
                if current_time - self.last_sample_time >= self.sample_interval_seconds:
                    # é‡‡æ ·å½“å‰æ—¶é—´çª—å£çš„æ•°æ®ï¼ˆ< 0.1msï¼‰
                    total_cached = sum(len(v) for v in self.window_data.values())
                    sampled_data = self._sample_window_data()
                    
                    if sampled_data:
                        self.stats['samples_taken'] += len(sampled_data)
                        logger.info(f"ğŸ“Š [å†å²è®°å½•] é‡‡æ ·å®Œæˆ: ä» {total_cached} æ¡ç¼“å­˜ä¸­é‡‡æ · {len(sampled_data)} æ¡æ•°æ®")
                        
                        # éé˜»å¡æ”¾å…¥å†™å…¥é˜Ÿåˆ—
                        try:
                            self.write_queue.put_nowait(sampled_data)
                            logger.info(f"âœ… [å†å²è®°å½•] å·²æ”¾å…¥å†™å…¥é˜Ÿåˆ—ï¼Œé˜Ÿåˆ—å¤§å°: {self.write_queue.qsize()}")
                        except asyncio.QueueFull:
                            # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒï¼ˆä¸å½±å“æ ¸å¿ƒæµç¨‹ï¼‰
                            self.stats['queue_drops'] += 1
                            logger.warning(f"âš ï¸  [å†å²è®°å½•] å†™å…¥é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæ•°æ®")
                    else:
                        if total_cached > 0:
                            logger.warning(f"âš ï¸  [å†å²è®°å½•] é‡‡æ ·å™¨è¿è¡Œï¼Œä½†é‡‡æ ·ç»“æœä¸ºç©ºï¼ˆç¼“å­˜ä¸­æœ‰ {total_cached} æ¡æ•°æ®ï¼‰")
                        else:
                            logger.debug(f"ğŸ’¤ [å†å²è®°å½•] é‡‡æ ·å™¨è¿è¡Œï¼Œä½†ç¼“å­˜ä¸ºç©ºï¼ˆæœªæ£€æµ‹åˆ°å¥—åˆ©æœºä¼šï¼‰")
                    
                    # æ¸…ç©ºæ—¶é—´çª—å£ç¼“å­˜
                    self.window_data.clear()
                    self.last_sample_time = current_time
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
                logger.error(f"âš ï¸  [å†å²è®°å½•] é‡‡æ ·é”™è¯¯ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
    
    def _sample_window_data(self) -> List[dict]:
        """é‡‡æ ·æ—¶é—´çª—å£æ•°æ®ï¼ˆ< 0.1msï¼‰
        
        ğŸ”¥ ä¿®æ”¹ï¼šæŒ‰ (symbol, exchange_buy, exchange_sell) åˆ†ç»„é‡‡æ ·
        ç¡®ä¿åŒä¸€ä¸ªsymbolçš„2ä¸ªæ–¹å‘ä»·å·®éƒ½èƒ½è¢«æ­£ç¡®é‡‡æ ·å’Œä¿å­˜
        """
        sampled = []
        
        # ğŸ”¥ æŒ‰ (symbol, exchange_buy, exchange_sell) åˆ†ç»„
        grouped_by_direction = defaultdict(list)
        
        for symbol, data_list in self.window_data.items():
            if not data_list:
                continue
            
            # æŒ‰æ–¹å‘åˆ†ç»„
            for data in data_list:
                exchange_buy = data.get('exchange_buy')
                exchange_sell = data.get('exchange_sell')
                if exchange_buy and exchange_sell:
                    direction_key = (symbol, exchange_buy, exchange_sell)
                    grouped_by_direction[direction_key].append(data)
        
        # å¯¹æ¯ä¸ªæ–¹å‘ç‹¬ç«‹é‡‡æ ·
        for (symbol, exchange_buy, exchange_sell), data_list in grouped_by_direction.items():
            if not data_list:
                continue
            
            if self.sample_strategy == "max":
                # é€‰æ‹©ä»·å·®æœ€å¤§çš„æ•°æ®ç‚¹ï¼ˆç»å¯¹å€¼æœ€å¤§ï¼‰
                best = max(data_list, key=lambda x: abs(x.get('spread_pct', 0)))
                # ğŸ”¥ ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆé¿å…Decimalç±»å‹ï¼‰
                best = self._convert_data_types(best)
                sampled.append(best)
            elif self.sample_strategy == "mean":
                # è®¡ç®—å¹³å‡å€¼
                # ğŸ”¥ è®¡ç®—èµ„é‡‘è´¹ç‡å¹³å‡å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                funding_rate_buy_list = [d.get('funding_rate_buy') for d in data_list if d.get('funding_rate_buy') is not None]
                funding_rate_sell_list = [d.get('funding_rate_sell') for d in data_list if d.get('funding_rate_sell') is not None]
                funding_rate_diff_list = [d.get('funding_rate_diff') for d in data_list if d.get('funding_rate_diff') is not None]
                
                avg_data = {
                    'symbol': symbol,
                    'timestamp': datetime.now().isoformat(),
                    'exchange_buy': exchange_buy,
                    'exchange_sell': exchange_sell,
                    'price_buy': sum(d.get('price_buy', 0) for d in data_list) / len(data_list),
                    'price_sell': sum(d.get('price_sell', 0) for d in data_list) / len(data_list),
                    'spread_pct': sum(d.get('spread_pct', 0) for d in data_list) / len(data_list),  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šä»·å·®ç™¾åˆ†æ¯”
                    'funding_rate_buy': sum(funding_rate_buy_list) / len(funding_rate_buy_list) if funding_rate_buy_list else None,  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šä¹°å…¥äº¤æ˜“æ‰€èµ„é‡‘è´¹ç‡
                    'funding_rate_sell': sum(funding_rate_sell_list) / len(funding_rate_sell_list) if funding_rate_sell_list else None,  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šå–å‡ºäº¤æ˜“æ‰€èµ„é‡‘è´¹ç‡
                    'funding_rate_diff': sum(funding_rate_diff_list) / len(funding_rate_diff_list) if funding_rate_diff_list else None,  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šèµ„é‡‘è´¹ç‡å·®ï¼ˆ8å°æ—¶è´¹ç‡å·®ï¼‰
                    'funding_rate_diff_annual': sum(d.get('funding_rate_diff_annual', 0) for d in data_list) / len(data_list) if data_list[0].get('funding_rate_diff_annual') is not None else None,
                    'size_buy': sum(d.get('size_buy', 0) for d in data_list) / len(data_list),
                    'size_sell': sum(d.get('size_sell', 0) for d in data_list) / len(data_list),
                }
                # ğŸ”¥ ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆé¿å…Decimalç±»å‹ï¼‰
                avg_data = self._convert_data_types(avg_data)
                sampled.append(avg_data)
            elif self.sample_strategy == "latest":
                # é€‰æ‹©æœ€æ–°çš„æ•°æ®ç‚¹
                latest = data_list[-1]
                # ğŸ”¥ ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆé¿å…Decimalç±»å‹ï¼‰
                latest = self._convert_data_types(latest)
                sampled.append(latest)
        
        return sampled
    
    def _convert_data_types(self, data: dict) -> dict:
        """è½¬æ¢æ•°æ®ä¸­çš„Decimalç±»å‹ä¸ºfloatï¼ˆç¡®ä¿SQLiteå…¼å®¹ï¼‰"""
        from decimal import Decimal
        converted = {}
        for key, value in data.items():
            if value is None:
                converted[key] = None
            elif isinstance(value, Decimal):
                converted[key] = float(value)
            elif isinstance(value, (int, float)):
                converted[key] = float(value)
            else:
                converted[key] = value
        return converted
    
    async def _init_sqlite_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“å’Œè¡¨ç»“æ„"""
        if not self.sqlite_enabled:
            return
        
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # åˆ›å»ºè¡¨
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS spread_history_sampled (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME NOT NULL,
                        symbol TEXT NOT NULL,
                        exchange_buy TEXT NOT NULL,
                        exchange_sell TEXT NOT NULL,
                        price_buy REAL NOT NULL,
                        price_sell REAL NOT NULL,
                        spread_pct REAL NOT NULL,
                        funding_rate_buy REAL,
                        funding_rate_sell REAL,
                        funding_rate_diff REAL,
                        funding_rate_diff_annual REAL,
                        size_buy REAL NOT NULL,
                        size_sell REAL NOT NULL,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # ğŸ”¥ æ£€æŸ¥å¹¶æ·»åŠ æ–°å­—æ®µï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†å­—æ®µä¸å­˜åœ¨ï¼‰
                try:
                    await db.execute("ALTER TABLE spread_history_sampled ADD COLUMN funding_rate_buy REAL")
                except Exception:
                    pass  # å­—æ®µå·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                
                try:
                    await db.execute("ALTER TABLE spread_history_sampled ADD COLUMN funding_rate_sell REAL")
                except Exception:
                    pass  # å­—æ®µå·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                
                try:
                    await db.execute("ALTER TABLE spread_history_sampled ADD COLUMN funding_rate_diff REAL")
                except Exception:
                    pass  # å­—æ®µå·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                
                # åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_sampled_timestamp 
                    ON spread_history_sampled(timestamp)
                """)
                
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_sampled_symbol 
                    ON spread_history_sampled(symbol)
                """)
                
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_sampled_symbol_timestamp 
                    ON spread_history_sampled(symbol, timestamp)
                """)
                
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_sampled_spread_pct 
                    ON spread_history_sampled(spread_pct)
                """)
                
                await db.commit()
                
        except Exception as e:
            logger.error(f"âš ï¸  [å†å²è®°å½•] SQLiteæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
            self.sqlite_enabled = False
    
    async def _async_write_loop(self):
        """å¼‚æ­¥å†™å…¥å¾ªç¯ï¼ˆç‹¬ç«‹ä»»åŠ¡ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰"""
        batch = []
        last_write_time = time.time()
        
        while self.running:
            try:
                # ç­‰å¾…æ•°æ®æˆ–è¶…æ—¶
                timeout = self.batch_timeout - (time.time() - last_write_time)
                if timeout <= 0:
                    timeout = 0.1
                
                data = await asyncio.wait_for(
                    self.write_queue.get(),
                    timeout=timeout
                )
                
                # dataå¯èƒ½æ˜¯å•ä¸ªdictæˆ–list
                if isinstance(data, list):
                    batch.extend(data)
                else:
                    batch.append(data)
                
                # æ‰¹é‡å†™å…¥ï¼ˆç§¯ç´¯batch_sizeæ¡æˆ–batch_timeoutç§’ï¼‰
                if len(batch) >= self.batch_size:
                    await self._write_batch(batch)
                    batch = []
                    last_write_time = time.time()
                    
            except asyncio.TimeoutError:
                # è¶…æ—¶ï¼Œå†™å…¥å½“å‰æ‰¹æ¬¡
                if batch:
                    await self._write_batch(batch)
                    batch = []
                    last_write_time = time.time()
            except asyncio.CancelledError:
                break
            except Exception as e:
                # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
                logger.error(f"âš ï¸  [å†å²è®°å½•] å†™å…¥é”™è¯¯ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
    
    async def _write_batch(self, batch: List[dict]):
        """æ‰¹é‡å†™å…¥CSVå’ŒSQLiteï¼ˆå¼‚æ­¥IOï¼Œä¸é˜»å¡ï¼‰"""
        if not batch:
            return
        
        logger.info(f"ğŸ’¾ [å†å²è®°å½•] å¼€å§‹æ‰¹é‡å†™å…¥ {len(batch)} æ¡æ•°æ®åˆ°CSVå’ŒSQLite...")
        
        # å¹¶è¡Œå†™å…¥CSVå’ŒSQLite
        tasks = [self._write_batch_to_csv(batch)]
        if self.sqlite_enabled:
            tasks.append(self._write_batch_to_sqlite(batch))
        
        await asyncio.gather(*tasks, return_exceptions=True)
        self.stats['batches_written'] += 1
    
    async def _write_batch_to_csv(self, batch: List[dict]):
        """æ‰¹é‡å†™å…¥CSVæ–‡ä»¶ï¼ˆå¼‚æ­¥IOï¼‰"""
        if not batch:
            return
        
        try:
            # æ£€æŸ¥æ—¥æœŸå˜åŒ–ï¼ˆæ–‡ä»¶è½®è½¬ï¼‰
            today = datetime.now().date()
            if today != self.current_date:
                self.current_date = today
                self.current_file = None
            
            # è·å–ä»Šå¤©çš„æ–‡ä»¶è·¯å¾„
            if self.current_file is None:
                csv_file = self.data_dir / "raw" / f"{today.strftime('%Y-%m-%d')}.csv"
                self.current_file = csv_file
                
                # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
                if not csv_file.exists():
                    header = "timestamp,symbol,exchange_buy,exchange_sell,price_buy,price_sell,spread_pct,funding_rate_buy,funding_rate_sell,funding_rate_diff,funding_rate_diff_annual,size_buy,size_sell\n"
                    async with aiofiles.open(csv_file, 'a') as f:
                        await f.write(header)
            
            # æ‰¹é‡å†™å…¥æ•°æ®
            lines = []
            for data in batch:
                line = (
                    f"{data.get('timestamp', '')},"
                    f"{data.get('symbol', '')},"
                    f"{data.get('exchange_buy', '')},"
                    f"{data.get('exchange_sell', '')},"
                    f"{data.get('price_buy', 0):.8f},"
                    f"{data.get('price_sell', 0):.8f},"
                    f"{data.get('spread_pct', 0):.6f},"
                    f"{data.get('funding_rate_buy', 0) if data.get('funding_rate_buy') is not None else 0:.8f},"
                    f"{data.get('funding_rate_sell', 0) if data.get('funding_rate_sell') is not None else 0:.8f},"
                    f"{data.get('funding_rate_diff', 0) if data.get('funding_rate_diff') is not None else 0:.8f},"
                    f"{data.get('funding_rate_diff_annual', 0) if data.get('funding_rate_diff_annual') is not None else 0:.2f},"
                    f"{data.get('size_buy', 0):.8f},"
                    f"{data.get('size_sell', 0):.8f}\n"
                )
                lines.append(line)
            
            # å¼‚æ­¥å†™å…¥æ–‡ä»¶
            async with aiofiles.open(self.current_file, 'a') as f:
                await f.writelines(lines)
            
        except Exception as e:
            # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
            logger.error(f"âš ï¸  [å†å²è®°å½•] CSVå†™å…¥é”™è¯¯ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
    
    async def _write_batch_to_sqlite(self, batch: List[dict]):
        """æ‰¹é‡å†™å…¥SQLiteæ•°æ®åº“ï¼ˆå¼‚æ­¥ï¼‰"""
        if not batch or not self.sqlite_enabled:
            return
        
        logger.info(f"ğŸ’¾ [å†å²è®°å½•] å†™å…¥SQLite: {len(batch)} æ¡æ•°æ®")
        
        try:
            # ğŸ”¥ è½¬æ¢ decimal.Decimal ä¸º floatï¼ˆSQLiteä¸æ”¯æŒDecimalç±»å‹ï¼‰
            def convert_value(v):
                """è½¬æ¢å€¼ç±»å‹ï¼Œç¡®ä¿SQLiteå…¼å®¹"""
                if v is None:
                    return None
                from decimal import Decimal
                if isinstance(v, Decimal):
                    return float(v)
                # ç¡®ä¿æ‰€æœ‰æ•°å€¼ç±»å‹éƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                if isinstance(v, (int, float)):
                    return float(v)
                return v
            
            async with aiosqlite.connect(self.db_path) as db:
                # ä½¿ç”¨executemanyæ‰¹é‡æ’å…¥ï¼Œæé«˜æ€§èƒ½
                await db.executemany("""
                    INSERT INTO spread_history_sampled 
                    (timestamp, symbol, exchange_buy, exchange_sell, 
                     price_buy, price_sell, spread_pct, 
                     funding_rate_buy, funding_rate_sell, funding_rate_diff, funding_rate_diff_annual, 
                     size_buy, size_sell)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, [
                    (
                        data.get('timestamp', ''),
                        data.get('symbol', ''),
                        data.get('exchange_buy', ''),
                        data.get('exchange_sell', ''),
                        convert_value(data.get('price_buy', 0)),
                        convert_value(data.get('price_sell', 0)),
                        convert_value(data.get('spread_pct', 0)),  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šä»·å·®ç™¾åˆ†æ¯”
                        convert_value(data.get('funding_rate_buy')) if data.get('funding_rate_buy') is not None else None,  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šä¹°å…¥äº¤æ˜“æ‰€èµ„é‡‘è´¹ç‡
                        convert_value(data.get('funding_rate_sell')) if data.get('funding_rate_sell') is not None else None,  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šå–å‡ºäº¤æ˜“æ‰€èµ„é‡‘è´¹ç‡
                        convert_value(data.get('funding_rate_diff')) if data.get('funding_rate_diff') is not None else None,  # ğŸ”¥ ä¸»è¦æ•°æ®ï¼šèµ„é‡‘è´¹ç‡å·®ï¼ˆ8å°æ—¶è´¹ç‡å·®ï¼‰
                        convert_value(data.get('funding_rate_diff_annual')) if data.get('funding_rate_diff_annual') is not None else None,
                        convert_value(data.get('size_buy', 0)),
                        convert_value(data.get('size_sell', 0)),
                    )
                    for data in batch
                ])
                await db.commit()
                self.stats['sqlite_batches_written'] += 1
                
        except Exception as e:
            # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
            logger.error(f"âš ï¸  [å†å²è®°å½•] SQLiteå†™å…¥é”™è¯¯ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
    
    async def _flush_remaining_data(self):
        """åˆ·æ–°å‰©ä½™æ•°æ®"""
        # é‡‡æ ·å‰©ä½™æ•°æ®
        sampled_data = self._sample_window_data()
        if sampled_data:
            try:
                self.write_queue.put_nowait(sampled_data)
            except asyncio.QueueFull:
                pass
        
        # å†™å…¥é˜Ÿåˆ—ä¸­å‰©ä½™çš„æ•°æ®
        batch = []
        while not self.write_queue.empty():
            try:
                data = self.write_queue.get_nowait()
                if isinstance(data, list):
                    batch.extend(data)
                else:
                    batch.append(data)
            except asyncio.QueueEmpty:
                break
        
        if batch:
            await self._write_batch(batch)
    
    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯ï¼ˆå®šæœŸå‹ç¼©å’Œå½’æ¡£æ—§æ–‡ä»¶ + æ¸…ç†æ•°æ®åº“ï¼‰"""
        # å¯åŠ¨æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ‰§è¡Œç¬¬ä¸€æ¬¡æ¸…ç†
        await asyncio.sleep(3600)  # ç­‰å¾…1å°æ—¶
        
        while self.running:
            try:
                # 1. å½’æ¡£æ—§æ–‡ä»¶ï¼ˆå‹ç¼©å’Œå½’æ¡£ï¼‰
                await self._archive_old_files()
                
                # 2. ğŸ”¥ æ¸…ç†æ•°æ®åº“æ—§æ•°æ®ï¼ˆä¿æŒæ€§èƒ½ï¼‰
                await self._cleanup_database()
                
                # ç­‰å¾…æ¸…ç†é—´éš”
                await asyncio.sleep(self.cleanup_interval_hours * 3600)
            except asyncio.CancelledError:
                break
            except Exception as e:
                # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
                logger.error(f"âš ï¸  [å†å²è®°å½•] æ¸…ç†ä»»åŠ¡é”™è¯¯ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
                await asyncio.sleep(3600)  # å‡ºé”™åç­‰å¾…1å°æ—¶å†é‡è¯•
    
    async def _archive_old_files(self):
        """å½’æ¡£æ—§æ–‡ä»¶ï¼ˆå‹ç¼©å’Œå½’æ¡£ï¼‰"""
        import gzip
        import shutil
        
        raw_dir = self.data_dir / "raw"
        archive_dir = self.data_dir / "archive"
        
        if not raw_dir.exists():
            return
        
        compress_cutoff = datetime.now().date() - timedelta(days=self.compress_after_days)
        archive_cutoff = datetime.now().date() - timedelta(days=self.archive_after_days)
        
        compressed_count = 0
        archived_count = 0
        
        for csv_file in raw_dir.glob("*.csv"):
            try:
                # æå–æ—¥æœŸï¼ˆæ–‡ä»¶åæ ¼å¼ï¼šYYYY-MM-DD.csvï¼‰
                file_date_str = csv_file.stem
                file_date = datetime.strptime(file_date_str, "%Y-%m-%d").date()
                
                if file_date < archive_cutoff:
                    # å½’æ¡£ï¼šç§»åŠ¨åˆ°å½’æ¡£ç›®å½•å¹¶å‹ç¼©
                    gz_file = archive_dir / f"{file_date_str}.csv.gz"
                    
                    # å¼‚æ­¥å‹ç¼©ï¼ˆä½¿ç”¨çº¿ç¨‹æ± é¿å…é˜»å¡ï¼‰
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(
                        None,
                        self._compress_file,
                        csv_file,
                        gz_file
                    )
                    
                    csv_file.unlink()
                    archived_count += 1
                    
                elif file_date < compress_cutoff:
                    # å‹ç¼©ï¼šåŸåœ°å‹ç¼©
                    gz_file = csv_file.with_suffix('.csv.gz')
                    
                    # å¼‚æ­¥å‹ç¼©
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(
                        None,
                        self._compress_file,
                        csv_file,
                        gz_file
                    )
                    
                    csv_file.unlink()
                    compressed_count += 1
                    
            except ValueError:
                # æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡
                continue
            except Exception as e:
                # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
                logger.error(f"âš ï¸  [å†å²è®°å½•] æ–‡ä»¶å¤„ç†é”™è¯¯ï¼ˆå·²éš”ç¦»ï¼‰: {csv_file.name}, {e}", exc_info=True)
                continue
        
        if compressed_count > 0 or archived_count > 0:
            self.stats['files_compressed'] += compressed_count
            self.stats['files_archived'] += archived_count
            logger.info(f"ğŸ“¦ [å†å²è®°å½•] æ¸…ç†å®Œæˆ: å‹ç¼© {compressed_count} ä¸ªæ–‡ä»¶ï¼Œå½’æ¡£ {archived_count} ä¸ªæ–‡ä»¶")
    
    async def _cleanup_database(self):
        """æ¸…ç†æ•°æ®åº“æ—§æ•°æ®ï¼ˆåˆ é™¤è¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®ï¼‰
        
        ä¸ºäº†ä¿æŒæ•°æ®åº“æ€§èƒ½ï¼Œå®šæœŸåˆ é™¤è¶…è¿‡ä¿ç•™æœŸï¼ˆé»˜è®¤48å°æ—¶ï¼‰çš„å†å²æ•°æ®ã€‚
        """
        if not self.sqlite_enabled or not self.db_path.exists():
            return
        
        try:
            cutoff_time = datetime.now() - timedelta(hours=self.db_retention_hours)
            
            async with aiosqlite.connect(self.db_path) as db:
                # 1. æŸ¥è¯¢å³å°†åˆ é™¤çš„æ•°æ®é‡
                async with db.execute("""
                    SELECT COUNT(*) FROM spread_history_sampled
                    WHERE timestamp < ?
                """, (cutoff_time.isoformat(),)) as cursor:
                    row = await cursor.fetchone()
                    rows_to_delete = row[0] if row else 0
                
                if rows_to_delete == 0:
                    logger.debug(
                        f"ğŸ—‘ï¸  [æ•°æ®åº“æ¸…ç†] æ— éœ€æ¸…ç†ï¼ˆæ‰€æœ‰æ•°æ®å‡åœ¨ä¿ç•™æœŸå†…ï¼š{self.db_retention_hours}å°æ—¶ï¼‰"
                    )
                    return
                
                # 2. åˆ é™¤æ—§æ•°æ®
                async with db.execute("""
                    DELETE FROM spread_history_sampled
                    WHERE timestamp < ?
                """, (cutoff_time.isoformat(),)) as cursor:
                    await db.commit()
                
                # 3. ä¼˜åŒ–æ•°æ®åº“ï¼ˆé‡Šæ”¾ç©ºé—´ï¼‰
                await db.execute("VACUUM")
                await db.commit()
                
                # 4. æŸ¥è¯¢æ¸…ç†åçš„æ•°æ®é‡
                async with db.execute("""
                    SELECT COUNT(*) FROM spread_history_sampled
                """) as cursor:
                    row = await cursor.fetchone()
                    remaining_rows = row[0] if row else 0
                
                logger.info(
                    f"ğŸ—‘ï¸  [æ•°æ®åº“æ¸…ç†] æ¸…ç†å®Œæˆ\n"
                    f"   åˆ é™¤è®°å½•: {rows_to_delete:,} æ¡\n"
                    f"   ä¿ç•™è®°å½•: {remaining_rows:,} æ¡\n"
                    f"   ä¿ç•™æ—¶é•¿: {self.db_retention_hours}å°æ—¶\n"
                    f"   æˆªæ­¢æ—¶é—´: {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}"
                )
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats['db_cleanup_count'] = self.stats.get('db_cleanup_count', 0) + 1
                self.stats['db_rows_deleted'] = self.stats.get('db_rows_deleted', 0) + rows_to_delete
                
        except Exception as e:
            # é”™è¯¯éš”ç¦»ï¼šä¸å½±å“æ ¸å¿ƒæµç¨‹
            logger.error(f"âš ï¸  [æ•°æ®åº“æ¸…ç†] æ¸…ç†å¤±è´¥ï¼ˆå·²éš”ç¦»ï¼‰: {e}", exc_info=True)
    
    @staticmethod
    def _compress_file(source_file: Path, target_file: Path):
        """å‹ç¼©æ–‡ä»¶ï¼ˆåŒæ­¥æ“ä½œï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        import gzip
        import shutil
        
        with open(source_file, 'rb') as f_in:
            with gzip.open(target_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'queue_size': self.write_queue.qsize(),
            'window_data_count': sum(len(v) for v in self.window_data.values()),
        }

